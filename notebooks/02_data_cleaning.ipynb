{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧹 Data Cleaning and Preprocessing\n",
    "## APS Failure Dataset - Class Imbalance Preprocessing Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 **Notebook Overview**\n",
    "This notebook implements comprehensive data preprocessing for the severely imbalanced APS failure dataset. We'll create a robust preprocessing pipeline that handles missing values, feature scaling, and prepares the data for class imbalance techniques.\n",
    "\n",
    "### 📋 **Processing Steps**\n",
    "1. **Data Loading and Initial Setup**\n",
    "2. **Missing Values Handling**\n",
    "3. **Feature Engineering and Selection**\n",
    "4. **Data Scaling and Normalization**\n",
    "5. **Train-Test Split with Stratification**\n",
    "6. **Data Validation and Quality Checks**\n",
    "7. **Preprocessing Pipeline Testing**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# 🎨 Set Style and Configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 📊 Configure Plotting Parameters\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# 📁 Create directories for artifacts\n",
    "os.makedirs('artifacts/data_transformation', exist_ok=True)\n",
    "os.makedirs('artifacts/preprocessors', exist_ok=True)\n",
    "\n",
    "print(\"✅ Libraries imported and directories created successfully!\")\n",
    "print(f\"📅 Processing Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1️⃣ Data Loading and Initial Setup\n",
    "\n",
    "Load the dataset and perform initial data assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📁 Load Dataset\n",
    "print(\"🔄 Loading APS Failure Training Dataset...\")\n",
    "data = pd.read_csv('aps_failure_training_set.csv', na_values=['na'])\n",
    "\n",
    "print(f\"✅ Dataset loaded successfully!\")\n",
    "print(f\"📏 Original Dataset Shape: {data.shape}\")\n",
    "print(f\"💾 Memory Usage: {data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# 🎯 Separate features and target\n",
    "X = data.drop('class', axis=1)\n",
    "y = data['class']\n",
    "\n",
    "print(f\"\\n🎯 Features shape: {X.shape}\")\n",
    "print(f\"🎯 Target shape: {y.shape}\")\n",
    "print(f\"🎯 Target classes: {y.unique()}\")\n",
    "print(f\"🎯 Class distribution: {y.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Initial Data Quality Assessment\n",
    "print(\"📊 INITIAL DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Missing values analysis\n",
    "missing_counts = X.isnull().sum()\n",
    "missing_percentages = (missing_counts / len(X)) * 100\n",
    "missing_info = pd.DataFrame({\n",
    "    'Missing_Count': missing_counts,\n",
    "    'Missing_Percentage': missing_percentages\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "columns_with_missing = missing_info[missing_info['Missing_Count'] > 0]\n",
    "\n",
    "print(f\"📊 Total features: {len(X.columns)}\")\n",
    "print(f\"🔍 Features with missing values: {len(columns_with_missing)}\")\n",
    "print(f\"✅ Features without missing values: {len(X.columns) - len(columns_with_missing)}\")\n",
    "\n",
    "if len(columns_with_missing) > 0:\n",
    "    print(f\"\\n🔍 Top 10 features with most missing values:\")\n",
    "    display(columns_with_missing.head(10))\n",
    "\n",
    "# Data types analysis\n",
    "print(f\"\\n📊 Data Types Distribution:\")\n",
    "print(X.dtypes.value_counts())\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\n📈 Basic Statistics:\")\n",
    "print(f\"  • Min value across all features: {X.min().min():.2e}\")\n",
    "print(f\"  • Max value across all features: {X.max().max():.2e}\")\n",
    "print(f\"  • Mean of feature means: {X.mean().mean():.2e}\")\n",
    "print(f\"  • Mean of feature std: {X.std().mean():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Missing Values Handling\n",
    "\n",
    "Implement comprehensive missing values handling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 Missing Values Handling Strategy\n",
    "print(\"🔧 MISSING VALUES HANDLING STRATEGY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Analyze missing data patterns\n",
    "def analyze_missing_patterns(df):\n",
    "    \"\"\"\n",
    "    Analyze missing data patterns in the dataset\n",
    "    \"\"\"\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_percent = (missing_data / len(df)) * 100\n",
    "    \n",
    "    # Categorize columns by missing percentage\n",
    "    no_missing = missing_percent[missing_percent == 0].index.tolist()\n",
    "    low_missing = missing_percent[(missing_percent > 0) & (missing_percent <= 10)].index.tolist()\n",
    "    medium_missing = missing_percent[(missing_percent > 10) & (missing_percent <= 50)].index.tolist()\n",
    "    high_missing = missing_percent[missing_percent > 50].index.tolist()\n",
    "    \n",
    "    return {\n",
    "        'no_missing': no_missing,\n",
    "        'low_missing': low_missing,\n",
    "        'medium_missing': medium_missing,\n",
    "        'high_missing': high_missing,\n",
    "        'missing_percent': missing_percent\n",
    "    }\n",
    "\n",
    "# Analyze patterns\n",
    "missing_analysis = analyze_missing_patterns(X)\n",
    "\n",
    "print(f\"📊 Missing Data Pattern Analysis:\")\n",
    "print(f\"  • No missing values: {len(missing_analysis['no_missing'])} features\")\n",
    "print(f\"  • Low missing (≤10%): {len(missing_analysis['low_missing'])} features\")\n",
    "print(f\"  • Medium missing (10-50%): {len(missing_analysis['medium_missing'])} features\")\n",
    "print(f\"  • High missing (>50%): {len(missing_analysis['high_missing'])} features\")\n",
    "\n",
    "# Display high missing features\n",
    "if len(missing_analysis['high_missing']) > 0:\n",
    "    print(f\"\\n⚠️ Features with >50% missing values:\")\n",
    "    high_missing_info = missing_analysis['missing_percent'][missing_analysis['high_missing']].sort_values(ascending=False)\n",
    "    for feature, pct in high_missing_info.items():\n",
    "        print(f\"    {feature}: {pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Visualize Missing Data Patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "fig.suptitle('🔍 Missing Data Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Missing data heatmap (sample of columns)\n",
    "ax1 = axes[0, 0]\n",
    "sample_cols = X.columns[:50]  # First 50 columns for visualization\n",
    "missing_matrix = X[sample_cols].isnull()\n",
    "sns.heatmap(missing_matrix.T, cbar=True, ax=ax1, cmap='viridis', \n",
    "           yticklabels=True, xticklabels=False)\n",
    "ax1.set_title('Missing Data Heatmap (Sample Features)', fontweight='bold')\n",
    "ax1.set_xlabel('Samples')\n",
    "ax1.set_ylabel('Features')\n",
    "\n",
    "# 2. Missing percentage distribution\n",
    "ax2 = axes[0, 1]\n",
    "missing_pcts = missing_analysis['missing_percent'][missing_analysis['missing_percent'] > 0]\n",
    "ax2.hist(missing_pcts, bins=20, color='orange', alpha=0.7, edgecolor='black')\n",
    "ax2.set_xlabel('Missing Percentage (%)')\n",
    "ax2.set_ylabel('Number of Features')\n",
    "ax2.set_title('Distribution of Missing Percentages', fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Missing data by category\n",
    "ax3 = axes[1, 0]\n",
    "categories = ['No Missing', 'Low (≤10%)', 'Medium (10-50%)', 'High (>50%)']\n",
    "counts = [len(missing_analysis['no_missing']), \n",
    "          len(missing_analysis['low_missing']),\n",
    "          len(missing_analysis['medium_missing']), \n",
    "          len(missing_analysis['high_missing'])]\n",
    "colors = ['#2ECC71', '#F39C12', '#E67E22', '#E74C3C']\n",
    "\n",
    "bars = ax3.bar(categories, counts, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax3.set_ylabel('Number of Features')\n",
    "ax3.set_title('Features by Missing Data Category', fontweight='bold')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, count in zip(bars, counts):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "             f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 4. Top missing features\n",
    "ax4 = axes[1, 1]\n",
    "if len(columns_with_missing) > 0:\n",
    "    top_missing = columns_with_missing.head(15)\n",
    "    ax4.barh(range(len(top_missing)), top_missing['Missing_Percentage'], \n",
    "             color='red', alpha=0.7)\n",
    "    ax4.set_yticks(range(len(top_missing)))\n",
    "    ax4.set_yticklabels(top_missing.index)\n",
    "    ax4.set_xlabel('Missing Percentage (%)')\n",
    "    ax4.set_title('Top 15 Features with Missing Values', fontweight='bold')\n",
    "    ax4.grid(axis='x', alpha=0.3)\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'No Missing Values Found', ha='center', va='center',\n",
    "             transform=ax4.transAxes, fontsize=14, fontweight='bold')\n",
    "    ax4.set_title('Missing Values Status', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifacts/missing_data_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"💾 Missing data analysis saved to: artifacts/missing_data_analysis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 Implement Missing Values Handling\n",
    "print(\"🔧 IMPLEMENTING MISSING VALUES HANDLING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create a copy of the data for processing\n",
    "X_processed = X.copy()\n",
    "\n",
    "# Strategy: Use median imputation for numeric features\n",
    "# This is robust to outliers and works well with skewed distributions\n",
    "\n",
    "print(\"📊 Before imputation:\")\n",
    "print(f\"  • Total missing values: {X_processed.isnull().sum().sum():,}\")\n",
    "print(f\"  • Features with missing values: {(X_processed.isnull().sum() > 0).sum()}\")\n",
    "\n",
    "# Apply median imputation\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(X_processed),\n",
    "    columns=X_processed.columns,\n",
    "    index=X_processed.index\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 After imputation:\")\n",
    "print(f\"  • Total missing values: {X_imputed.isnull().sum().sum():,}\")\n",
    "print(f\"  • Features with missing values: {(X_imputed.isnull().sum() > 0).sum()}\")\n",
    "\n",
    "# Save the imputer\n",
    "joblib.dump(imputer, 'artifacts/preprocessors/imputer.pkl')\n",
    "print(f\"\\n💾 Imputer saved to: artifacts/preprocessors/imputer.pkl\")\n",
    "\n",
    "# Verify no missing values remain\n",
    "if X_imputed.isnull().sum().sum() == 0:\n",
    "    print(\"✅ All missing values successfully handled!\")\n",
    "else:\n",
    "    print(\"⚠️ Some missing values remain - investigation needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Feature Engineering and Selection\n",
    "\n",
    "Remove low-variance features and perform feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 Feature Engineering and Selection\n",
    "print(\"🔧 FEATURE ENGINEERING AND SELECTION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Identify constant features (zero variance)\n",
    "print(\"🔍 Identifying constant and low-variance features...\")\n",
    "\n",
    "# Check for constant features\n",
    "constant_features = []\n",
    "for col in X_imputed.columns:\n",
    "    if X_imputed[col].nunique() <= 1:\n",
    "        constant_features.append(col)\n",
    "\n",
    "print(f\"📊 Constant features found: {len(constant_features)}\")\n",
    "if len(constant_features) > 0:\n",
    "    print(f\"  • Features: {constant_features[:10]}...\")  # Show first 10\n",
    "\n",
    "# 2. Remove low-variance features\n",
    "# Use VarianceThreshold to remove features with very low variance\n",
    "variance_threshold = VarianceThreshold(threshold=0.01)  # Remove features with variance < 0.01\n",
    "X_variance_filtered = variance_threshold.fit_transform(X_imputed)\n",
    "\n",
    "# Get feature names after variance filtering\n",
    "variance_feature_mask = variance_threshold.get_support()\n",
    "selected_features = X_imputed.columns[variance_feature_mask].tolist()\n",
    "removed_features = X_imputed.columns[~variance_feature_mask].tolist()\n",
    "\n",
    "print(f\"\\n📊 Variance-based feature selection:\")\n",
    "print(f\"  • Original features: {len(X_imputed.columns)}\")\n",
    "print(f\"  • Features kept: {len(selected_features)}\")\n",
    "print(f\"  • Features removed: {len(removed_features)}\")\n",
    "\n",
    "# Create DataFrame with selected features\n",
    "X_selected = pd.DataFrame(\n",
    "    X_variance_filtered,\n",
    "    columns=selected_features,\n",
    "    index=X_imputed.index\n",
    ")\n",
    "\n",
    "# Save variance threshold selector\n",
    "joblib.dump(variance_threshold, 'artifacts/preprocessors/variance_selector.pkl')\n",
    "print(f\"\\n💾 Variance selector saved to: artifacts/preprocessors/variance_selector.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Feature Selection Analysis Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('🔧 Feature Engineering Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Feature variance distribution (before filtering)\n",
    "ax1 = axes[0, 0]\n",
    "variances = X_imputed.var()\n",
    "ax1.hist(np.log10(variances + 1e-10), bins=50, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "ax1.set_xlabel('Log10(Variance)')\n",
    "ax1.set_ylabel('Number of Features')\n",
    "ax1.set_title('Feature Variance Distribution (Before)', fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.axvline(np.log10(0.01), color='red', linestyle='--', label='Threshold')\n",
    "ax1.legend()\n",
    "\n",
    "# 2. Feature variance distribution (after filtering)\n",
    "ax2 = axes[0, 1]\n",
    "selected_variances = X_selected.var()\n",
    "ax2.hist(np.log10(selected_variances + 1e-10), bins=50, color='lightgreen', alpha=0.7, edgecolor='black')\n",
    "ax2.set_xlabel('Log10(Variance)')\n",
    "ax2.set_ylabel('Number of Features')\n",
    "ax2.set_title('Feature Variance Distribution (After)', fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Feature selection summary\n",
    "ax3 = axes[1, 0]\n",
    "categories = ['Kept Features', 'Removed Features']\n",
    "counts = [len(selected_features), len(removed_features)]\n",
    "colors = ['#2ECC71', '#E74C3C']\n",
    "\n",
    "bars = ax3.bar(categories, counts, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax3.set_ylabel('Number of Features')\n",
    "ax3.set_title('Feature Selection Summary', fontweight='bold')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, count in zip(bars, counts):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "             f'{count}', ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "\n",
    "# 4. Sample of removed features (if any)\n",
    "ax4 = axes[1, 1]\n",
    "if len(removed_features) > 0:\n",
    "    # Show variance of removed features\n",
    "    removed_variances = X_imputed[removed_features].var().sort_values(ascending=False)\n",
    "    top_removed = removed_variances.head(15)\n",
    "    \n",
    "    ax4.barh(range(len(top_removed)), top_removed.values, color='red', alpha=0.6)\n",
    "    ax4.set_yticks(range(len(top_removed)))\n",
    "    ax4.set_yticklabels(top_removed.index)\n",
    "    ax4.set_xlabel('Variance')\n",
    "    ax4.set_title('Top 15 Removed Features (by Variance)', fontweight='bold')\n",
    "    ax4.grid(axis='x', alpha=0.3)\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'No Features Removed', ha='center', va='center',\n",
    "             transform=ax4.transAxes, fontsize=14, fontweight='bold')\n",
    "    ax4.set_title('Removed Features', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifacts/feature_engineering_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"💾 Feature engineering analysis saved to: artifacts/feature_engineering_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Data Scaling and Normalization\n",
    "\n",
    "Apply feature scaling to normalize the data range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 Data Scaling and Normalization\n",
    "print(\"🔧 DATA SCALING AND NORMALIZATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Analyze feature scales before scaling\n",
    "print(\"📊 Feature scale analysis (before scaling):\")\n",
    "feature_stats = X_selected.describe()\n",
    "print(f\"  • Min value: {feature_stats.loc['min'].min():.2e}\")\n",
    "print(f\"  • Max value: {feature_stats.loc['max'].max():.2e}\")\n",
    "print(f\"  • Mean of means: {feature_stats.loc['mean'].mean():.2e}\")\n",
    "print(f\"  • Mean of std: {feature_stats.loc['std'].mean():.2e}\")\n",
    "\n",
    "# Apply StandardScaler (z-score normalization)\n",
    "# This is preferred for algorithms like SVM and Logistic Regression\n",
    "print(f\"\\n🔄 Applying StandardScaler...\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_selected),\n",
    "    columns=X_selected.columns,\n",
    "    index=X_selected.index\n",
    ")\n",
    "\n",
    "# Analyze feature scales after scaling\n",
    "print(\"\\n📊 Feature scale analysis (after scaling):\")\n",
    "scaled_stats = X_scaled.describe()\n",
    "print(f\"  • Min value: {scaled_stats.loc['min'].min():.2e}\")\n",
    "print(f\"  • Max value: {scaled_stats.loc['max'].max():.2e}\")\n",
    "print(f\"  • Mean of means: {scaled_stats.loc['mean'].mean():.2e}\")\n",
    "print(f\"  • Mean of std: {scaled_stats.loc['std'].mean():.2e}\")\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, 'artifacts/preprocessors/scaler.pkl')\n",
    "print(f\"\\n💾 Scaler saved to: artifacts/preprocessors/scaler.pkl\")\n",
    "\n",
    "print(\"✅ Data scaling completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Scaling Analysis Visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('🔧 Data Scaling Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Select sample features for visualization\n",
    "sample_features = X_selected.columns[:6]\n",
    "\n",
    "for i, feature in enumerate(sample_features):\n",
    "    row, col = i // 3, i % 3\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Plot before and after scaling\n",
    "    ax.hist(X_selected[feature], bins=30, alpha=0.6, label='Before Scaling', \n",
    "           color='red', density=True)\n",
    "    ax.hist(X_scaled[feature], bins=30, alpha=0.6, label='After Scaling', \n",
    "           color='blue', density=True)\n",
    "    \n",
    "    ax.set_title(f'{feature}', fontweight='bold')\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifacts/scaling_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"💾 Scaling analysis saved to: artifacts/scaling_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Train-Test Split with Stratification\n",
    "\n",
    "Split the data while maintaining class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 Encode Target Variable\n",
    "print(\"🎯 TARGET VARIABLE ENCODING\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Encode target labels: 'neg' -> 0, 'pos' -> 1\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"📊 Original target classes: {y.unique()}\")\n",
    "print(f\"📊 Encoded target classes: {np.unique(y_encoded)}\")\n",
    "print(f\"📊 Label mapping: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
    "\n",
    "# Save label encoder\n",
    "joblib.dump(label_encoder, 'artifacts/preprocessors/label_encoder.pkl')\n",
    "print(f\"💾 Label encoder saved to: artifacts/preprocessors/label_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔄 Train-Test Split with Stratification\n",
    "print(\"🔄 TRAIN-TEST SPLIT WITH STRATIFICATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Perform stratified split to maintain class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"📊 Data split completed:\")\n",
    "print(f\"  • Training set: {X_train.shape} features, {len(y_train)} samples\")\n",
    "print(f\"  • Testing set: {X_test.shape} features, {len(y_test)} samples\")\n",
    "\n",
    "# Verify class distribution is maintained\n",
    "train_class_dist = pd.Series(y_train).value_counts(normalize=True).sort_index()\n",
    "test_class_dist = pd.Series(y_test).value_counts(normalize=True).sort_index()\n",
    "\n",
    "print(f\"\\n📊 Class distribution verification:\")\n",
    "print(f\"  • Training set - Class 0: {train_class_dist[0]:.3f}, Class 1: {train_class_dist[1]:.3f}\")\n",
    "print(f\"  • Testing set - Class 0: {test_class_dist[0]:.3f}, Class 1: {test_class_dist[1]:.3f}\")\n",
    "\n",
    "# Calculate class imbalance ratio\n",
    "train_counts = pd.Series(y_train).value_counts().sort_index()\n",
    "test_counts = pd.Series(y_test).value_counts().sort_index()\n",
    "\n",
    "train_ratio = train_counts[0] / train_counts[1]\n",
    "test_ratio = test_counts[0] / test_counts[1]\n",
    "\n",
    "print(f\"\\n⚖️ Imbalance ratios:\")\n",
    "print(f\"  • Training set: {train_ratio:.1f}:1\")\n",
    "print(f\"  • Testing set: {test_ratio:.1f}:1\")\n",
    "\n",
    "if abs(train_ratio - test_ratio) < 0.5:\n",
    "    print(\"✅ Class distribution successfully maintained across splits!\")\n",
    "else:\n",
    "    print(\"⚠️ Class distribution varies between splits - check stratification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Train-Test Split Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('🔄 Train-Test Split Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Training set class distribution\n",
    "ax1 = axes[0, 0]\n",
    "train_class_counts = pd.Series(y_train).value_counts().sort_index()\n",
    "bars1 = ax1.bar(['Negative (0)', 'Positive (1)'], train_class_counts.values, \n",
    "               color=['#3498DB', '#E74C3C'], alpha=0.8, edgecolor='black')\n",
    "ax1.set_ylabel('Number of Samples')\n",
    "ax1.set_title('Training Set Class Distribution', fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add count labels\n",
    "for bar, count in zip(bars1, train_class_counts.values):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "             f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Testing set class distribution\n",
    "ax2 = axes[0, 1]\n",
    "test_class_counts = pd.Series(y_test).value_counts().sort_index()\n",
    "bars2 = ax2.bar(['Negative (0)', 'Positive (1)'], test_class_counts.values, \n",
    "               color=['#3498DB', '#E74C3C'], alpha=0.8, edgecolor='black')\n",
    "ax2.set_ylabel('Number of Samples')\n",
    "ax2.set_title('Testing Set Class Distribution', fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add count labels\n",
    "for bar, count in zip(bars2, test_class_counts.values):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "             f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. Split size comparison\n",
    "ax3 = axes[1, 0]\n",
    "split_sizes = [len(X_train), len(X_test)]\n",
    "split_labels = ['Training Set', 'Testing Set']\n",
    "colors = ['#2ECC71', '#F39C12']\n",
    "\n",
    "bars3 = ax3.bar(split_labels, split_sizes, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax3.set_ylabel('Number of Samples')\n",
    "ax3.set_title('Train-Test Split Size', fontweight='bold')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add size labels\n",
    "for bar, size in zip(bars3, split_sizes):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "             f'{size:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 4. Class proportion comparison\n",
    "ax4 = axes[1, 1]\n",
    "class_props = pd.DataFrame({\n",
    "    'Training': train_class_dist.values,\n",
    "    'Testing': test_class_dist.values\n",
    "}, index=['Negative (0)', 'Positive (1)'])\n",
    "\n",
    "class_props.plot(kind='bar', ax=ax4, color=['#3498DB', '#E74C3C'], \n",
    "                alpha=0.8, edgecolor='black')\n",
    "ax4.set_ylabel('Proportion')\n",
    "ax4.set_title('Class Proportions Comparison', fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "ax4.set_xticklabels(ax4.get_xticklabels(), rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifacts/train_test_split_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"💾 Train-test split analysis saved to: artifacts/train_test_split_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Data Validation and Quality Checks\n",
    "\n",
    "Perform final validation of the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Data Validation and Quality Checks\n",
    "print(\"✅ DATA VALIDATION AND QUALITY CHECKS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def validate_processed_data(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Comprehensive validation of processed data\n",
    "    \"\"\"\n",
    "    validation_results = {}\n",
    "    \n",
    "    # 1. Check for missing values\n",
    "    train_missing = X_train.isnull().sum().sum()\n",
    "    test_missing = X_test.isnull().sum().sum()\n",
    "    validation_results['missing_values'] = train_missing + test_missing == 0\n",
    "    \n",
    "    # 2. Check data types\n",
    "    train_numeric = X_train.select_dtypes(include=[np.number]).shape[1]\n",
    "    validation_results['all_numeric'] = train_numeric == X_train.shape[1]\n",
    "    \n",
    "    # 3. Check feature consistency\n",
    "    validation_results['feature_consistency'] = list(X_train.columns) == list(X_test.columns)\n",
    "    \n",
    "    # 4. Check scaling (mean close to 0, std close to 1)\n",
    "    train_means = X_train.mean()\n",
    "    train_stds = X_train.std()\n",
    "    validation_results['proper_scaling'] = (\n",
    "        abs(train_means.mean()) < 0.1 and \n",
    "        abs(train_stds.mean() - 1.0) < 0.1\n",
    "    )\n",
    "    \n",
    "    # 5. Check class distribution preservation\n",
    "    train_ratio = pd.Series(y_train).value_counts()[0] / pd.Series(y_train).value_counts()[1]\n",
    "    test_ratio = pd.Series(y_test).value_counts()[0] / pd.Series(y_test).value_counts()[1]\n",
    "    validation_results['class_distribution_preserved'] = abs(train_ratio - test_ratio) < 1.0\n",
    "    \n",
    "    # 6. Check for infinite values\n",
    "    train_inf = np.isinf(X_train).sum().sum()\n",
    "    test_inf = np.isinf(X_test).sum().sum()\n",
    "    validation_results['no_infinite_values'] = train_inf + test_inf == 0\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Run validation\n",
    "validation_results = validate_processed_data(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(\"🔍 Validation Results:\")\n",
    "for check, result in validation_results.items():\n",
    "    status = \"✅ PASS\" if result else \"❌ FAIL\"\n",
    "    print(f\"  • {check.replace('_', ' ').title()}: {status}\")\n",
    "\n",
    "# Overall validation score\n",
    "total_checks = len(validation_results)\n",
    "passed_checks = sum(validation_results.values())\n",
    "validation_score = (passed_checks / total_checks) * 100\n",
    "\n",
    "print(f\"\\n📊 Overall Validation Score: {validation_score:.1f}% ({passed_checks}/{total_checks} checks passed)\")\n",
    "\n",
    "if validation_score == 100:\n",
    "    print(\"🎉 All validation checks passed! Data is ready for modeling.\")\n",
    "elif validation_score >= 80:\n",
    "    print(\"✅ Most validation checks passed. Minor issues may need attention.\")\n",
    "else:\n",
    "    print(\"⚠️ Several validation checks failed. Review preprocessing steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Final Data Summary\n",
    "print(\"📊 FINAL PREPROCESSED DATA SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"🎯 Final Dataset Characteristics:\")\n",
    "print(f\"  • Total samples: {len(X_scaled):,}\")\n",
    "print(f\"  • Total features: {X_scaled.shape[1]}\")\n",
    "print(f\"  • Training samples: {len(X_train):,} ({len(X_train)/len(X_scaled)*100:.1f}%)\")\n",
    "print(f\"  • Testing samples: {len(X_test):,} ({len(X_test)/len(X_scaled)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n🔧 Preprocessing Steps Applied:\")\n",
    "print(f\"  • ✅ Missing values imputed (median strategy)\")\n",
    "print(f\"  • ✅ Low-variance features removed ({len(removed_features)} features)\")\n",
    "print(f\"  • ✅ Features scaled (StandardScaler)\")\n",
    "print(f\"  • ✅ Target variable encoded (LabelEncoder)\")\n",
    "print(f\"  • ✅ Stratified train-test split (80/20)\")\n",
    "\n",
    "print(f\"\\n📁 Saved Preprocessing Objects:\")\n",
    "print(f\"  • artifacts/preprocessors/imputer.pkl\")\n",
    "print(f\"  • artifacts/preprocessors/variance_selector.pkl\")\n",
    "print(f\"  • artifacts/preprocessors/scaler.pkl\")\n",
    "print(f\"  • artifacts/preprocessors/label_encoder.pkl\")\n",
    "\n",
    "print(f\"\\n⚖️ Class Imbalance Status:\")\n",
    "train_neg_count = (y_train == 0).sum()\n",
    "train_pos_count = (y_train == 1).sum()\n",
    "imbalance_ratio = train_neg_count / train_pos_count\n",
    "print(f\"  • Training set imbalance: {imbalance_ratio:.1f}:1\")\n",
    "print(f\"  • Negative class: {train_neg_count:,} samples\")\n",
    "print(f\"  • Positive class: {train_pos_count:,} samples\")\n",
    "print(f\"  • 🚨 Severe imbalance requires specialized techniques!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7️⃣ Save Processed Data\n",
    "\n",
    "Save the preprocessed data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💾 Save Processed Data\n",
    "print(\"💾 SAVING PROCESSED DATA\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Save processed datasets\n",
    "X_train.to_csv('artifacts/data_transformation/X_train_processed.csv', index=False)\n",
    "X_test.to_csv('artifacts/data_transformation/X_test_processed.csv', index=False)\n",
    "pd.Series(y_train).to_csv('artifacts/data_transformation/y_train_processed.csv', index=False, header=['target'])\n",
    "pd.Series(y_test).to_csv('artifacts/data_transformation/y_test_processed.csv', index=False, header=['target'])\n",
    "\n",
    "# Save feature names\n",
    "feature_info = {\n",
    "    'original_features': list(X.columns),\n",
    "    'selected_features': list(X_selected.columns),\n",
    "    'removed_features': removed_features,\n",
    "    'final_features': list(X_train.columns)\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('artifacts/data_transformation/feature_info.json', 'w') as f:\n",
    "    json.dump(feature_info, f, indent=2)\n",
    "\n",
    "print(f\"✅ Processed data saved:\")\n",
    "print(f\"  • Training features: artifacts/data_transformation/X_train_processed.csv\")\n",
    "print(f\"  • Testing features: artifacts/data_transformation/X_test_processed.csv\")\n",
    "print(f\"  • Training targets: artifacts/data_transformation/y_train_processed.csv\")\n",
    "print(f\"  • Testing targets: artifacts/data_transformation/y_test_processed.csv\")\n",
    "print(f\"  • Feature information: artifacts/data_transformation/feature_info.json\")\n",
    "\n",
    "print(f\"\\n🎯 Data preprocessing pipeline completed successfully!\")\n",
    "print(f\"📅 Completion time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Data Preprocessing Summary\n",
    "\n",
    "### ✅ **Completed Steps:**\n",
    "\n",
    "1. **🔍 Data Loading & Assessment**\n",
    "   - Loaded 60,000 samples with 171 features\n",
    "   - Identified severe class imbalance (59:1 ratio)\n",
    "   - Analyzed missing value patterns\n",
    "\n",
    "2. **🧹 Missing Values Handling**\n",
    "   - Applied median imputation strategy\n",
    "   - Preserved data distribution characteristics\n",
    "   - Saved imputer for future use\n",
    "\n",
    "3. **🔧 Feature Engineering**\n",
    "   - Removed low-variance features\n",
    "   - Applied variance threshold filtering\n",
    "   - Optimized feature space for modeling\n",
    "\n",
    "4. **📏 Data Scaling**\n",
    "   - Applied StandardScaler normalization\n",
    "   - Ensured zero mean and unit variance\n",
    "   - Prepared data for scale-sensitive algorithms\n",
    "\n",
    "5. **🎯 Target Encoding & Splitting**\n",
    "   - Encoded categorical target to numeric\n",
    "   - Performed stratified train-test split\n",
    "   - Maintained class distribution across splits\n",
    "\n",
    "### 📊 **Final Dataset Characteristics:**\n",
    "- **Training Set**: 48,000 samples\n",
    "- **Testing Set**: 12,000 samples  \n",
    "- **Features**: Optimized feature count after preprocessing\n",
    "- **Class Balance**: Preserved severe imbalance for realistic evaluation\n",
    "\n",
    "### 💡 **Next Steps:**\n",
    "1. **Model Training** with class imbalance techniques\n",
    "2. **SMOTE Application** for synthetic oversampling\n",
    "3. **Class Weight Optimization** for algorithms\n",
    "4. **Ensemble Methods** for robust predictions\n",
    "\n",
    "---\n",
    "\n",
    "**🚀 Ready for Model Training Phase!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
