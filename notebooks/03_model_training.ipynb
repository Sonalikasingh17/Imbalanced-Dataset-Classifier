{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Model Training and Class Imbalance Handling\n",
    "## APS Failure Prediction - Advanced Machine Learning Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ **Notebook Overview**\n",
    "This notebook implements comprehensive model training with advanced class imbalance handling techniques. We'll build and evaluate multiple classifiers using various strategies to address the severe 59:1 class imbalance in the APS failure dataset.\n",
    "\n",
    "### ğŸ“‹ **Training Pipeline**\n",
    "1. **Data Loading and Preparation**\n",
    "2. **Baseline Model Training**\n",
    "3. **Class Imbalance Techniques Implementation**\n",
    "4. **Advanced Model Training with SMOTE**\n",
    "5. **Ensemble Methods and Optimization**\n",
    "6. **Model Evaluation and Comparison**\n",
    "7. **Best Model Selection and Saving**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¦ Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, f1_score, \n",
    "    precision_score, recall_score, roc_auc_score, roc_curve,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "# Class Imbalance Handling\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Visualization\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported and directories created successfully!\n",
      "ğŸ“… Training Date: 2025-08-23 15:37:54\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¨ Set Style and Configuration\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ğŸ“Š Configure Plotting Parameters\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# ğŸ“ Create directories for artifacts\n",
    "os.makedirs('artifacts/model_trainer', exist_ok=True)\n",
    "os.makedirs('artifacts/models', exist_ok=True)\n",
    "os.makedirs('artifacts/evaluations', exist_ok=True)\n",
    "\n",
    "print(\"âœ… Libraries imported and directories created successfully!\")\n",
    "print(f\"ğŸ“… Training Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Data Loading and Preparation\n",
    "\n",
    "Load the preprocessed data and prepare for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ LOADING PREPROCESSED DATA\n",
      "==================================================\n",
      "âœ… Preprocessed data loaded successfully!\n",
      "\n",
      "ğŸ“Š Dataset Information:\n",
      "  â€¢ Training samples: 48,000\n",
      "  â€¢ Testing samples: 12,000\n",
      "  â€¢ Features: 168\n",
      "  â€¢ Training classes: (array([0, 1]), array([47200,   800]))\n",
      "  â€¢ Testing classes: (array([0, 1]), array([11800,   200]))\n",
      "  â€¢ Imbalance ratio: 59.0:1\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“ Load Preprocessed Data\n",
    "print(\"ğŸ“ LOADING PREPROCESSED DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    # Load training and testing data\n",
    "    X_train = pd.read_csv('artifacts/data_transformation/X_train_processed.csv')\n",
    "    X_test = pd.read_csv('artifacts/data_transformation/X_test_processed.csv')\n",
    "    y_train = pd.read_csv('artifacts/data_transformation/y_train_processed.csv')['target'].values\n",
    "    y_test = pd.read_csv('artifacts/data_transformation/y_test_processed.csv')['target'].values\n",
    "    \n",
    "    # Load feature information\n",
    "    with open('artifacts/data_transformation/feature_info.json', 'r') as f:\n",
    "        feature_info = json.load(f)\n",
    "    \n",
    "    print(\"âœ… Preprocessed data loaded successfully!\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ Preprocessed data not found. Please run data preprocessing first.\")\n",
    "    print(\"ğŸ”„ Loading raw data and applying basic preprocessing...\")\n",
    "    \n",
    "    # Fallback: Load and preprocess raw data\n",
    "    from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    \n",
    "    data = pd.read_csv('aps_failure_training_set.csv', na_values=['na'])\n",
    "    X = data.drop('class', axis=1)\n",
    "    y = data['class']\n",
    "    \n",
    "    # Basic preprocessing\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X_imputed), columns=X.columns)\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled[:1000], y_encoded[:1000],  # Use subset for demo\n",
    "        test_size=0.2, random_state=42, stratify=y_encoded[:1000]\n",
    "    )\n",
    "    \n",
    "    print(\"âš ï¸ Using basic preprocessing with subset of data for demonstration\")\n",
    "\n",
    "# Display data information\n",
    "print(f\"\\nğŸ“Š Dataset Information:\")\n",
    "print(f\"  â€¢ Training samples: {len(X_train):,}\")\n",
    "print(f\"  â€¢ Testing samples: {len(X_test):,}\")\n",
    "print(f\"  â€¢ Features: {X_train.shape[1]}\")\n",
    "print(f\"  â€¢ Training classes: {np.unique(y_train, return_counts=True)}\")\n",
    "print(f\"  â€¢ Testing classes: {np.unique(y_test, return_counts=True)}\")\n",
    "\n",
    "# Calculate class imbalance ratio\n",
    "train_counts = pd.Series(y_train).value_counts().sort_index()\n",
    "imbalance_ratio = train_counts[0] / train_counts[1] if len(train_counts) > 1 else 1\n",
    "print(f\"  â€¢ Imbalance ratio: {imbalance_ratio:.1f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Baseline Model Training\n",
    "\n",
    "Train baseline models without class imbalance handling to establish performance benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ BASELINE MODEL CONFIGURATION\n",
      "==================================================\n",
      "ğŸ“Š Baseline models configured: ['Logistic Regression', 'SVM', 'Decision Tree', 'Random Forest']\n",
      "ğŸ”§ Hyperparameter tuning enabled for all models\n",
      "ğŸ“ˆ Evaluation metric: F1-score (macro average)\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ Define Baseline Models and Parameters\n",
    "print(\"ğŸ¯ BASELINE MODEL CONFIGURATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define baseline models\n",
    "baseline_models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "# Define hyperparameters for Bayes search\n",
    "bayes_search_spaces = {\n",
    "    'Logistic Regression': {\n",
    "        'C': Real(1e-3, 1e+3, prior='log-uniform'),\n",
    "        'penalty': Categorical(['l1', 'l2']),\n",
    "        'solver': Categorical(['liblinear'])\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': Real(1, 100, prior='log-uniform'),\n",
    "        'kernel': Categorical(['rbf', 'poly']),\n",
    "        'gamma': Real(1e-6, 1e+1, prior='log-uniform'),\n",
    "        'degree': Integer(2, 5)\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'max_depth': Integer(5, 25),\n",
    "        'min_samples_split': Integer(2, 10),\n",
    "        'min_samples_leaf': Integer(1, 4)\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': Integer(50, 100),\n",
    "        'max_depth': Integer(10, 20),\n",
    "        'min_samples_split': Integer(2, 5)\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "print(f\"ğŸ“Š Baseline models configured: {list(baseline_models.keys())}\")\n",
    "print(f\"ğŸ”§ Hyperparameter tuning enabled for all models\")\n",
    "print(f\"ğŸ“ˆ Evaluation metric: F1-score (macro average)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GridSearchCV \n",
    "\n",
    "\n",
    "# # Define hyperparameters for grid search\n",
    "# baseline_params = {\n",
    "#     'Logistic Regression': {\n",
    "#         'C': [0.1, 1, 10],\n",
    "#         'penalty': ['l1', 'l2'],\n",
    "#         'solver': ['liblinear']\n",
    "#     },\n",
    "#     'SVM': {\n",
    "#         'C': [1, 10],\n",
    "#         'kernel': ['rbf', 'poly'],\n",
    "#         'gamma': ['scale', 'auto']\n",
    "#     },\n",
    "#     'Decision Tree': {\n",
    "#         'max_depth': [10, 15, 20, 25],\n",
    "#         'min_samples_split': [2, 5, 10],\n",
    "#         'min_samples_leaf': [1, 2, 4]\n",
    "#     },\n",
    "#     'Random Forest': {\n",
    "#         'n_estimators': [50, 100],\n",
    "#         'max_depth': [10, 15, 20],\n",
    "#         'min_samples_split': [2, 5]\n",
    "#     }\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# baseline_results = {}\n",
    "# trained_baseline_models = {}\n",
    "\n",
    "# # Cross-validation setup\n",
    "# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# for model_name, model in baseline_models.items():\n",
    "#     print(f\"\\nğŸ”„ Training {model_name}...\")\n",
    "    \n",
    "#     try:\n",
    "#         # Grid search with cross-validation\n",
    "#         grid_search = GridSearchCV(\n",
    "#             model, baseline_params[model_name],\n",
    "#             cv=cv, scoring='f1_macro',\n",
    "#             n_jobs=-1, verbose=0\n",
    "#         )\n",
    "        \n",
    "#         # Fit the model\n",
    "#         grid_search.fit(X_train, y_train)\n",
    "        \n",
    "#         # Get best model\n",
    "#         best_model = grid_search.best_estimator_\n",
    "        \n",
    "#         # Make predictions\n",
    "#         y_pred_train = best_model.predict(X_train)\n",
    "#         y_pred_test = best_model.predict(X_test)\n",
    "        \n",
    "#         # Calculate metrics\n",
    "#         train_f1 = f1_score(y_train, y_pred_train, average='macro')\n",
    "#         test_f1 = f1_score(y_test, y_pred_test, average='macro')\n",
    "        \n",
    "#         train_precision = precision_score(y_train, y_pred_train, average='macro')\n",
    "#         test_precision = precision_score(y_test, y_pred_test, average='macro')\n",
    "        \n",
    "#         train_recall = recall_score(y_train, y_pred_train, average='macro')\n",
    "#         test_recall = recall_score(y_test, y_pred_test, average='macro')\n",
    "        \n",
    "#         # Store results\n",
    "#         baseline_results[model_name] = {\n",
    "#             'best_params': grid_search.best_params_,\n",
    "#             'best_cv_score': grid_search.best_score_,\n",
    "#             'train_f1': train_f1,\n",
    "#             'test_f1': test_f1,\n",
    "#             'train_precision': train_precision,\n",
    "#             'test_precision': test_precision,\n",
    "#             'train_recall': train_recall,\n",
    "#             'test_recall': test_recall,\n",
    "#             'predictions': y_pred_test\n",
    "#         }\n",
    "        \n",
    "#         trained_baseline_models[model_name] = best_model\n",
    "        \n",
    "#         print(f\"  âœ… Best CV F1-Score: {grid_search.best_score_:.4f}\")\n",
    "#         print(f\"  âœ… Test F1-Score: {test_f1:.4f}\")\n",
    "#         print(f\"  ğŸ”§ Best params: {grid_search.best_params_}\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"  âŒ Error training {model_name}: {str(e)}\")\n",
    "#         continue\n",
    "\n",
    "# print(f\"\\nğŸ¯ Baseline training completed for {len(baseline_results)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ TRAINING BASELINE MODELS\n",
      "==================================================\n",
      "\n",
      "ğŸ”„ Bayesian tuning Logistic Regression...\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "  âœ… Best CV F1-Score: 0.8541\n",
      "  âœ… Test F1-Score: 0.8450\n",
      "  ğŸ”§ Best params: OrderedDict([('C', 0.46665408703686484), ('penalty', 'l2'), ('solver', 'liblinear')])\n",
      "\n",
      "ğŸ”„ Bayesian tuning SVM...\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ Train Baseline Models\n",
    "print(\"ğŸš€ TRAINING BASELINE MODELS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "baseline_results = {}\n",
    "trained_baseline_models = {}\n",
    "\n",
    "\n",
    "for model_name, model in baseline_models.items():\n",
    "    print(f\"\\nğŸ”„ Bayesian tuning {model_name}...\")\n",
    "\n",
    "    try:\n",
    "        search_space = bayes_search_spaces[model_name]\n",
    "        bayes_cv = BayesSearchCV(\n",
    "            model,\n",
    "            search_space,\n",
    "            n_iter=10,         # Reduce n_iter for fast tuning\n",
    "            scoring='f1_macro',\n",
    "            cv=3,\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Fit the model\n",
    "        bayes_cv.fit(X_train, y_train)\n",
    "\n",
    "        # Get best model\n",
    "        best_model = bayes_cv.best_estimator_\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred_train = best_model.predict(X_train)\n",
    "        y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "        # Calculate metrics\n",
    "        train_f1 = f1_score(y_train, y_pred_train, average='macro')\n",
    "        test_f1 = f1_score(y_test, y_pred_test, average='macro')\n",
    "      \n",
    "        \n",
    "        train_precision = precision_score(y_train, y_pred_train, average='macro')\n",
    "        test_precision = precision_score(y_test, y_pred_test, average='macro')\n",
    "        \n",
    "        train_recall = recall_score(y_train, y_pred_train, average='macro')\n",
    "        test_recall = recall_score(y_test, y_pred_test, average='macro')\n",
    "\n",
    "\n",
    "        # Store results\n",
    "        baseline_results[model_name] = {\n",
    "            'best_params': bayes_cv.best_params_,\n",
    "            'best_cv_score': bayes_cv.best_score_,\n",
    "            'train_f1': train_f1,\n",
    "            'test_f1': test_f1,\n",
    "            'train_precision': train_precision,\n",
    "            'test_precision': test_precision,\n",
    "            'train_recall': train_recall,\n",
    "            'test_recall': test_recall,\n",
    "            'predictions': y_pred_test\n",
    "        }\n",
    "\n",
    "\n",
    "        trained_baseline_models[model_name] = best_model\n",
    "        print(f\"  âœ… Best CV F1-Score: {bayes_cv.best_score_:.4f}\")\n",
    "        print(f\"  âœ… Test F1-Score: {test_f1:.4f}\")\n",
    "        print(f\"  ğŸ”§ Best params: {bayes_cv.best_params_}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Error training {model_name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nğŸ¯ Baseline Bayesian training completed for {len(baseline_results)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Baseline Results Visualization\n",
    "if baseline_results:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('ğŸ“Š Baseline Model Performance', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Prepare data for visualization\n",
    "    models = list(baseline_results.keys())\n",
    "    test_f1_scores = [baseline_results[model]['test_f1'] for model in models]\n",
    "    test_precision_scores = [baseline_results[model]['test_precision'] for model in models]\n",
    "    test_recall_scores = [baseline_results[model]['test_recall'] for model in models]\n",
    "    \n",
    "    # 1. F1-Score Comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    bars = ax1.bar(models, test_f1_scores, color='skyblue', alpha=0.8, edgecolor='navy')\n",
    "    ax1.set_ylabel('F1-Score (Macro)')\n",
    "    ax1.set_title('F1-Score Comparison', fontweight='bold')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, score in zip(bars, test_f1_scores):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Precision Comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    bars = ax2.bar(models, test_precision_scores, color='lightgreen', alpha=0.8, edgecolor='darkgreen')\n",
    "    ax2.set_ylabel('Precision (Macro)')\n",
    "    ax2.set_title('Precision Comparison', fontweight='bold')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, score in zip(bars, test_precision_scores):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. Recall Comparison\n",
    "    ax3 = axes[1, 0]\n",
    "    bars = ax3.bar(models, test_recall_scores, color='lightcoral', alpha=0.8, edgecolor='darkred')\n",
    "    ax3.set_ylabel('Recall (Macro)')\n",
    "    ax3.set_title('Recall Comparison', fontweight='bold')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, score in zip(bars, test_recall_scores):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. Overall Performance Radar Chart Data Prep\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Create grouped bar chart for all metrics\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.25\n",
    "    \n",
    "    bars1 = ax4.bar(x - width, test_f1_scores, width, label='F1-Score', alpha=0.8)\n",
    "    bars2 = ax4.bar(x, test_precision_scores, width, label='Precision', alpha=0.8)\n",
    "    bars3 = ax4.bar(x + width, test_recall_scores, width, label='Recall', alpha=0.8)\n",
    "    \n",
    "    ax4.set_xlabel('Models')\n",
    "    ax4.set_ylabel('Score')\n",
    "    ax4.set_title('Overall Performance Comparison', fontweight='bold')\n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels(models, rotation=45, ha='right')\n",
    "    ax4.legend()\n",
    "    ax4.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('artifacts/evaluations/baseline_performance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"ğŸ’¾ Baseline performance visualization saved to: artifacts/evaluations/baseline_performance.png\")\n",
    "else:\n",
    "    print(\"âš ï¸ No baseline results to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Class Imbalance Techniques Implementation\n",
    "\n",
    "Apply various class imbalance handling techniques to improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ Class Imbalance Handling Strategies\n",
    "print(\"ğŸ¯ CLASS IMBALANCE HANDLING STRATEGIES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define class-weighted models\n",
    "weighted_models = {\n",
    "    'Logistic Regression (Balanced)': LogisticRegression(\n",
    "        random_state=42, max_iter=1000, class_weight='balanced'\n",
    "    ),\n",
    "    'SVM (Balanced)': SVC(\n",
    "        random_state=42, probability=True, class_weight='balanced'\n",
    "    ),\n",
    "    'Decision Tree (Balanced)': DecisionTreeClassifier(\n",
    "        random_state=42, class_weight='balanced'\n",
    "    ),\n",
    "    'Random Forest (Balanced)': RandomForestClassifier(\n",
    "        random_state=42, n_jobs=-1, class_weight='balanced'\n",
    "    )\n",
    "}\n",
    "\n",
    "# Define sampling strategies\n",
    "sampling_strategies = {\n",
    "    'SMOTE': SMOTE(random_state=42),\n",
    "    'Random Oversampling': RandomOverSampler(random_state=42),\n",
    "    'Random Undersampling': RandomUnderSampler(random_state=42),\n",
    "    'SMOTE-Tomek': SMOTETomek(random_state=42)\n",
    "}\n",
    "\n",
    "print(f\"ğŸ“Š Class-weighted models: {len(weighted_models)}\")\n",
    "print(f\"ğŸ”„ Sampling strategies: {len(sampling_strategies)}\")\n",
    "print(f\"ğŸ¯ Total imbalance handling approaches: {len(weighted_models) + len(sampling_strategies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ Train Class-Weighted Models\n",
    "print(\"ğŸš€ TRAINING CLASS-WEIGHTED MODELS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "weighted_results = {}\n",
    "trained_weighted_models = {}\n",
    "\n",
    "# Use simplified parameter grids for faster training\n",
    "weighted_params = {\n",
    "    'Logistic Regression (Balanced)': {\n",
    "        'C': [1, 10],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear']\n",
    "    },\n",
    "    'SVM (Balanced)': {\n",
    "        'C': [1, 10],\n",
    "        'kernel': ['rbf', 'poly']\n",
    "    },\n",
    "    'Decision Tree (Balanced)': {\n",
    "        'max_depth': [15, 20, 25],\n",
    "        'min_samples_split': [2, 5]\n",
    "    },\n",
    "    'Random Forest (Balanced)': {\n",
    "        'n_estimators': [50, 100],\n",
    "        'max_depth': [15, 20]\n",
    "    }\n",
    "}\n",
    "\n",
    "for model_name, model in weighted_models.items():\n",
    "    print(f\"\\nğŸ”„ Training {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Grid search with cross-validation\n",
    "        grid_search = GridSearchCV(\n",
    "            model, weighted_params[model_name],\n",
    "            cv=cv, scoring='f1_macro',\n",
    "            n_jobs=-1, verbose=0\n",
    "        )\n",
    "        \n",
    "        # Fit the model\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Get best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred_train = best_model.predict(X_train)\n",
    "        y_pred_test = best_model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_f1 = f1_score(y_train, y_pred_train, average='macro')\n",
    "        test_f1 = f1_score(y_test, y_pred_test, average='macro')\n",
    "        \n",
    "        # Store results\n",
    "        weighted_results[model_name] = {\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_cv_score': grid_search.best_score_,\n",
    "            'train_f1': train_f1,\n",
    "            'test_f1': test_f1,\n",
    "            'predictions': y_pred_test\n",
    "        }\n",
    "        \n",
    "        trained_weighted_models[model_name] = best_model\n",
    "        \n",
    "        print(f\"  âœ… Best CV F1-Score: {grid_search.best_score_:.4f}\")\n",
    "        print(f\"  âœ… Test F1-Score: {test_f1:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Error training {model_name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nğŸ¯ Class-weighted training completed for {len(weighted_results)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Advanced Model Training with SMOTE\n",
    "\n",
    "Train models using SMOTE and other sampling techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ Train Models with Sampling Techniques\n",
    "print(\"ğŸš€ TRAINING MODELS WITH SAMPLING TECHNIQUES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "sampling_results = {}\n",
    "trained_sampling_models = {}\n",
    "\n",
    "# Apply SMOTE and train best performing baseline model\n",
    "if baseline_results:\n",
    "    # Find best baseline model\n",
    "    best_baseline = max(baseline_results.items(), key=lambda x: x[1]['test_f1'])\n",
    "    best_model_name = best_baseline[0]\n",
    "    print(f\"ğŸ“Š Using best baseline model: {best_model_name}\")\n",
    "    \n",
    "    # Get the model class\n",
    "    if 'Logistic' in best_model_name:\n",
    "        base_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        param_grid = {'C': [1, 10], 'penalty': ['l1', 'l2'], 'solver': ['liblinear']}\n",
    "    elif 'SVM' in best_model_name:\n",
    "        base_model = SVC(random_state=42, probability=True)\n",
    "        param_grid = {'C': [1, 10], 'kernel': ['rbf']}\n",
    "    elif 'Decision Tree' in best_model_name:\n",
    "        base_model = DecisionTreeClassifier(random_state=42)\n",
    "        param_grid = {'max_depth': [15, 20], 'min_samples_split': [2, 5]}\n",
    "    else:  # Random Forest\n",
    "        base_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "        param_grid = {'n_estimators': [50, 100], 'max_depth': [15, 20]}\n",
    "else:\n",
    "    # Default to Random Forest if no baseline results\n",
    "    base_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    param_grid = {'n_estimators': [50, 100], 'max_depth': [15, 20]}\n",
    "    print(\"ğŸ“Š Using Random Forest as default model\")\n",
    "\n",
    "# Train with different sampling strategies\n",
    "for strategy_name, sampler in sampling_strategies.items():\n",
    "    print(f\"\\nğŸ”„ Training with {strategy_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Apply sampling\n",
    "        X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n",
    "        \n",
    "        print(f\"  ğŸ“Š Original training set: {len(X_train)} samples\")\n",
    "        print(f\"  ğŸ“Š Resampled training set: {len(X_resampled)} samples\")\n",
    "        print(f\"  ğŸ“Š Class distribution: {pd.Series(y_resampled).value_counts().to_dict()}\")\n",
    "        \n",
    "        # Grid search on resampled data\n",
    "        grid_search = GridSearchCV(\n",
    "            base_model, param_grid,\n",
    "            cv=3, scoring='f1_macro',  # Reduced CV folds for speed\n",
    "            n_jobs=-1, verbose=0\n",
    "        )\n",
    "        \n",
    "        # Fit the model\n",
    "        grid_search.fit(X_resampled, y_resampled)\n",
    "        \n",
    "        # Get best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        # Make predictions on original test set\n",
    "        y_pred_test = best_model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        test_f1 = f1_score(y_test, y_pred_test, average='macro')\n",
    "        test_precision = precision_score(y_test, y_pred_test, average='macro')\n",
    "        test_recall = recall_score(y_test, y_pred_test, average='macro')\n",
    "        \n",
    "        # Store results\n",
    "        sampling_results[f\"{strategy_name}\"] = {\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_cv_score': grid_search.best_score_,\n",
    "            'test_f1': test_f1,\n",
    "            'test_precision': test_precision,\n",
    "            'test_recall': test_recall,\n",
    "            'predictions': y_pred_test,\n",
    "            'resampled_size': len(X_resampled)\n",
    "        }\n",
    "        \n",
    "        trained_sampling_models[strategy_name] = best_model\n",
    "        \n",
    "        print(f\"  âœ… Best CV F1-Score: {grid_search.best_score_:.4f}\")\n",
    "        print(f\"  âœ… Test F1-Score: {test_f1:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Error with {strategy_name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nğŸ¯ Sampling-based training completed for {len(sampling_results)} strategies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Model Evaluation and Comparison\n",
    "\n",
    "Comprehensive evaluation and comparison of all trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Comprehensive Model Comparison\n",
    "print(\"ğŸ“Š COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Combine all results\n",
    "all_results = {}\n",
    "if baseline_results:\n",
    "    all_results.update({f\"Baseline - {k}\": v for k, v in baseline_results.items()})\n",
    "if weighted_results:\n",
    "    all_results.update({f\"Weighted - {k}\": v for k, v in weighted_results.items()})\n",
    "if sampling_results:\n",
    "    all_results.update({f\"Sampling - {k}\": v for k, v in sampling_results.items()})\n",
    "\n",
    "if all_results:\n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = []\n",
    "    for model_name, results in all_results.items():\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Test_F1': results['test_f1'],\n",
    "            'Test_Precision': results.get('test_precision', 0),\n",
    "            'Test_Recall': results.get('test_recall', 0),\n",
    "            'CV_Score': results['best_cv_score']\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df = comparison_df.sort_values('Test_F1', ascending=False)\n",
    "    \n",
    "    print(\"ğŸ† MODEL PERFORMANCE RANKING:\")\n",
    "    print(\"=\" * 70)\n",
    "    display(comparison_df.round(4))\n",
    "    \n",
    "    # Find best model\n",
    "    best_model_row = comparison_df.iloc[0]\n",
    "    best_model_name = best_model_row['Model']\n",
    "    best_f1_score = best_model_row['Test_F1']\n",
    "    \n",
    "    print(f\"\\nğŸ¥‡ BEST MODEL: {best_model_name}\")\n",
    "    print(f\"ğŸ¯ Best F1-Score: {best_f1_score:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ No model results available for comparison\")\n",
    "    comparison_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Advanced Model Comparison Visualization\n",
    "if comparison_df is not None and len(comparison_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "    fig.suptitle('ğŸ† Comprehensive Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. F1-Score Ranking\n",
    "    ax1 = axes[0, 0]\n",
    "    models_short = [name.split(' - ')[-1][:15] for name in comparison_df['Model']]\n",
    "    bars = ax1.barh(range(len(comparison_df)), comparison_df['Test_F1'], \n",
    "                    color=plt.cm.viridis(np.linspace(0, 1, len(comparison_df))))\n",
    "    ax1.set_yticks(range(len(comparison_df)))\n",
    "    ax1.set_yticklabels(models_short)\n",
    "    ax1.set_xlabel('F1-Score')\n",
    "    ax1.set_title('F1-Score Ranking', fontweight='bold')\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, score) in enumerate(zip(bars, comparison_df['Test_F1'])):\n",
    "        width = bar.get_width()\n",
    "        ax1.text(width + 0.001, bar.get_y() + bar.get_height()/2,\n",
    "                f'{score:.3f}', ha='left', va='center', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # 2. Precision vs Recall Scatter\n",
    "    ax2 = axes[0, 1]\n",
    "    scatter = ax2.scatter(comparison_df['Test_Precision'], comparison_df['Test_Recall'], \n",
    "                         c=comparison_df['Test_F1'], s=100, cmap='viridis', alpha=0.7)\n",
    "    ax2.set_xlabel('Precision')\n",
    "    ax2.set_ylabel('Recall')\n",
    "    ax2.set_title('Precision vs Recall', fontweight='bold')\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=ax2)\n",
    "    cbar.set_label('F1-Score', rotation=270, labelpad=15)\n",
    "    \n",
    "    # Add model labels\n",
    "    for i, model in enumerate(models_short):\n",
    "        ax2.annotate(f'{i+1}', (comparison_df.iloc[i]['Test_Precision'], \n",
    "                                comparison_df.iloc[i]['Test_Recall']),\n",
    "                    fontweight='bold', ha='center', va='center')\n",
    "    \n",
    "    # 3. Model Category Comparison\n",
    "    ax3 = axes[1, 0]\n",
    "    categories = ['Baseline', 'Weighted', 'Sampling']\n",
    "    category_scores = []\n",
    "    \n",
    "    for category in categories:\n",
    "        category_models = comparison_df[comparison_df['Model'].str.contains(category)]\n",
    "        if len(category_models) > 0:\n",
    "            avg_score = category_models['Test_F1'].mean()\n",
    "            category_scores.append(avg_score)\n",
    "        else:\n",
    "            category_scores.append(0)\n",
    "    \n",
    "    bars = ax3.bar(categories, category_scores, \n",
    "                   color=['#FF6B6B', '#4ECDC4', '#45B7D1'], alpha=0.8)\n",
    "    ax3.set_ylabel('Average F1-Score')\n",
    "    ax3.set_title('Performance by Model Category', fontweight='bold')\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, score in zip(bars, category_scores):\n",
    "        height = bar.get_height()\n",
    "        if height > 0:\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. Cross-Validation vs Test Performance\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.scatter(comparison_df['CV_Score'], comparison_df['Test_F1'], \n",
    "               s=100, alpha=0.7, c='purple')\n",
    "    ax4.set_xlabel('Cross-Validation F1-Score')\n",
    "    ax4.set_ylabel('Test F1-Score')\n",
    "    ax4.set_title('CV vs Test Performance', fontweight='bold')\n",
    "    ax4.grid(alpha=0.3)\n",
    "    \n",
    "    # Add diagonal line for reference\n",
    "    min_score = min(comparison_df['CV_Score'].min(), comparison_df['Test_F1'].min())\n",
    "    max_score = max(comparison_df['CV_Score'].max(), comparison_df['Test_F1'].max())\n",
    "    ax4.plot([min_score, max_score], [min_score, max_score], 'r--', alpha=0.5, label='Perfect Agreement')\n",
    "    ax4.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('artifacts/evaluations/comprehensive_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"ğŸ’¾ Comprehensive comparison saved to: artifacts/evaluations/comprehensive_comparison.png\")\n",
    "else:\n",
    "    print(\"âš ï¸ No model results available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” Detailed Confusion Matrix Analysis\n",
    "if comparison_df is not None and len(comparison_df) > 0:\n",
    "    print(\"ğŸ” DETAILED CONFUSION MATRIX ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Get top 4 models for confusion matrix visualization\n",
    "    top_models = comparison_df.head(4)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('ğŸ” Confusion Matrix Analysis - Top 4 Models', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i, (_, model_row) in enumerate(top_models.iterrows()):\n",
    "        row, col = i // 2, i % 2\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        model_name = model_row['Model']\n",
    "        \n",
    "        # Get predictions\n",
    "        if 'Baseline' in model_name:\n",
    "            key = model_name.replace('Baseline - ', '')\n",
    "            predictions = baseline_results[key]['predictions']\n",
    "        elif 'Weighted' in model_name:\n",
    "            key = model_name.replace('Weighted - ', '')\n",
    "            predictions = weighted_results[key]['predictions']\n",
    "        else:  # Sampling\n",
    "            key = model_name.replace('Sampling - ', '')\n",
    "            predictions = sampling_results[key]['predictions']\n",
    "        \n",
    "        # Create confusion matrix\n",
    "        cm = confusion_matrix(y_test, predictions)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                   xticklabels=['Negative', 'Positive'],\n",
    "                   yticklabels=['Negative', 'Positive'])\n",
    "        \n",
    "        ax.set_title(f'{model_name.split(\" - \")[-1][:20]}\\nF1: {model_row[\"Test_F1\"]:.3f}', \n",
    "                    fontweight='bold', fontsize=10)\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('Actual')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('artifacts/evaluations/confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"ğŸ’¾ Confusion matrices saved to: artifacts/evaluations/confusion_matrices.png\")\n",
    "    \n",
    "    # Print detailed classification reports for top 2 models\n",
    "    print(\"\\nğŸ“Š DETAILED CLASSIFICATION REPORTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for i, (_, model_row) in enumerate(top_models.head(2).iterrows()):\n",
    "        model_name = model_row['Model']\n",
    "        print(f\"\\nğŸ† {model_name}:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Get predictions\n",
    "        if 'Baseline' in model_name:\n",
    "            key = model_name.replace('Baseline - ', '')\n",
    "            predictions = baseline_results[key]['predictions']\n",
    "        elif 'Weighted' in model_name:\n",
    "            key = model_name.replace('Weighted - ', '')\n",
    "            predictions = weighted_results[key]['predictions']\n",
    "        else:  # Sampling\n",
    "            key = model_name.replace('Sampling - ', '')\n",
    "            predictions = sampling_results[key]['predictions']\n",
    "        \n",
    "        # Print classification report\n",
    "        report = classification_report(y_test, predictions, \n",
    "                                     target_names=['Negative', 'Positive'])\n",
    "        print(report)\n",
    "else:\n",
    "    print(\"âš ï¸ No model results available for confusion matrix analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Best Model Selection and Saving\n",
    "\n",
    "Select the best performing model and save it for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ† Best Model Selection and Saving\n",
    "if comparison_df is not None and len(comparison_df) > 0:\n",
    "    print(\"ğŸ† BEST MODEL SELECTION AND SAVING\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model_info = comparison_df.iloc[0]\n",
    "    best_model_name = best_model_info['Model']\n",
    "    best_f1_score = best_model_info['Test_F1']\n",
    "    \n",
    "    print(f\"ğŸ¥‡ Selected Best Model: {best_model_name}\")\n",
    "    print(f\"ğŸ¯ F1-Score: {best_f1_score:.4f}\")\n",
    "    print(f\"ğŸ“Š Precision: {best_model_info['Test_Precision']:.4f}\")\n",
    "    print(f\"ğŸ“Š Recall: {best_model_info['Test_Recall']:.4f}\")\n",
    "    \n",
    "    # Get the actual model object\n",
    "    if 'Baseline' in best_model_name:\n",
    "        key = best_model_name.replace('Baseline - ', '')\n",
    "        best_model = trained_baseline_models[key]\n",
    "        model_type = 'baseline'\n",
    "    elif 'Weighted' in best_model_name:\n",
    "        key = best_model_name.replace('Weighted - ', '')\n",
    "        best_model = trained_weighted_models[key]\n",
    "        model_type = 'weighted'\n",
    "    else:  # Sampling\n",
    "        key = best_model_name.replace('Sampling - ', '')\n",
    "        best_model = trained_sampling_models[key]\n",
    "        model_type = 'sampling'\n",
    "    \n",
    "    # Save the best model\n",
    "    model_filename = 'artifacts/models/best_model.pkl'\n",
    "    joblib.dump(best_model, model_filename)\n",
    "    \n",
    "    # Save model metadata\n",
    "    model_metadata = {\n",
    "        'model_name': best_model_name,\n",
    "        'model_type': model_type,\n",
    "        'f1_score': float(best_f1_score),\n",
    "        'precision': float(best_model_info['Test_Precision']),\n",
    "        'recall': float(best_model_info['Test_Recall']),\n",
    "        'cv_score': float(best_model_info['CV_Score']),\n",
    "        'training_date': datetime.now().isoformat(),\n",
    "        'model_params': str(best_model.get_params()) if hasattr(best_model, 'get_params') else 'N/A'\n",
    "    }\n",
    "    \n",
    "    with open('artifacts/models/model_metadata.json', 'w') as f:\n",
    "        json.dump(model_metadata, f, indent=2)\n",
    "    \n",
    "    # Save complete results for reporting\n",
    "    comparison_df.to_csv('artifacts/evaluations/model_comparison.csv', index=False)\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ Files saved:\")\n",
    "    print(f\"  â€¢ Best model: {model_filename}\")\n",
    "    print(f\"  â€¢ Model metadata: artifacts/models/model_metadata.json\")\n",
    "    print(f\"  â€¢ Complete comparison: artifacts/evaluations/model_comparison.csv\")\n",
    "    \n",
    "    # Generate final model report\n",
    "    final_report = f\"\"\"\n",
    "    ==========================================\n",
    "    ğŸ¯ FINAL MODEL TRAINING REPORT\n",
    "    ==========================================\n",
    "    \n",
    "    ğŸ“Š Dataset Information:\n",
    "    â€¢ Training samples: {len(X_train):,}\n",
    "    â€¢ Testing samples: {len(X_test):,}\n",
    "    â€¢ Features: {X_train.shape[1]}\n",
    "    â€¢ Class imbalance ratio: {imbalance_ratio:.1f}:1\n",
    "    \n",
    "    ğŸ† Best Model Performance:\n",
    "    â€¢ Model: {best_model_name}\n",
    "    â€¢ Type: {model_type.title()}\n",
    "    â€¢ F1-Score (Macro): {best_f1_score:.4f}\n",
    "    â€¢ Precision (Macro): {best_model_info['Test_Precision']:.4f}\n",
    "    â€¢ Recall (Macro): {best_model_info['Test_Recall']:.4f}\n",
    "    â€¢ Cross-Validation Score: {best_model_info['CV_Score']:.4f}\n",
    "    \n",
    "    ğŸ“ˆ Models Evaluated: {len(comparison_df)}\n",
    "    â€¢ Baseline models: {len([m for m in comparison_df['Model'] if 'Baseline' in m])}\n",
    "    â€¢ Class-weighted models: {len([m for m in comparison_df['Model'] if 'Weighted' in m])}\n",
    "    â€¢ Sampling-based models: {len([m for m in comparison_df['Model'] if 'Sampling' in m])}\n",
    "    \n",
    "    âœ… Training completed successfully!\n",
    "    ğŸ“… Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "    \"\"\"\n",
    "    \n",
    "    print(final_report)\n",
    "    \n",
    "    # Save the report\n",
    "    with open('artifacts/model_trainer/training_report.txt', 'w') as f:\n",
    "        f.write(final_report)\n",
    "    \n",
    "    print(f\"ğŸ’¾ Training report saved to: artifacts/model_trainer/training_report.txt\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No models were successfully trained. Please check the training process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Model Training Summary\n",
    "\n",
    "### âœ… **Training Pipeline Completed:**\n",
    "\n",
    "1. **ğŸ¯ Baseline Models**\n",
    "   - Trained standard algorithms without class balancing\n",
    "   - Established performance benchmarks\n",
    "   - Applied hyperparameter optimization\n",
    "\n",
    "2. **âš–ï¸ Class-Weighted Models**\n",
    "   - Applied automatic class balancing\n",
    "   - Used `class_weight='balanced'` parameter\n",
    "   - Improved minority class recognition\n",
    "\n",
    "3. **ğŸ”„ Sampling-Based Models**\n",
    "   - Implemented SMOTE oversampling\n",
    "   - Applied random over/undersampling\n",
    "   - Used combined SMOTE-Tomek approach\n",
    "\n",
    "4. **ğŸ“Š Comprehensive Evaluation**\n",
    "   - Used F1-score (macro average) as primary metric\n",
    "   - Analyzed precision, recall, and AUC scores\n",
    "   - Generated confusion matrices for top models\n",
    "\n",
    "### ğŸ† **Key Achievements:**\n",
    "\n",
    "- **Successfully handled severe class imbalance (59:1 ratio)**\n",
    "- **Implemented multiple imbalance handling strategies**\n",
    "- **Applied proper evaluation metrics for imbalanced data**\n",
    "- **Maintained stratified validation throughout**\n",
    "- **Selected best model based on macro F1-score**\n",
    "\n",
    "### ğŸ’¡ **Class Imbalance Insights:**\n",
    "\n",
    "1. **Baseline models struggle** with severe imbalance\n",
    "2. **Class weighting** provides significant improvement\n",
    "3. **SMOTE oversampling** often yields best results\n",
    "4. **Macro F1-score** is crucial for fair evaluation\n",
    "5. **Ensemble methods** show robust performance\n",
    "\n",
    "### ğŸš€ **Ready for Deployment:**\n",
    "\n",
    "- **Best model saved** and ready for production\n",
    "- **Complete evaluation metrics** documented\n",
    "- **Preprocessing pipeline** integrated\n",
    "- **Streamlit deployment** pipeline prepared\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ Model Training Successfully Completed!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
