{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 Exploratory Data Analysis (EDA)\n",
    "## APS Failure Dataset - Class Imbalance Analysis\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 **Project Overview**\n",
    "This notebook performs comprehensive exploratory data analysis on the IDA2016 Challenge dataset for APS (Air Pressure System) failure prediction in Scania trucks. The dataset presents a severe class imbalance challenge with a 1:59 ratio between positive and negative classes.\n",
    "\n",
    "### 📋 **Notebook Contents**\n",
    "1. **Data Loading and Basic Information**\n",
    "2. **Class Distribution Analysis**\n",
    "3. **Missing Values Analysis**\n",
    "4. **Feature Distribution Analysis**\n",
    "5. **Correlation Analysis**\n",
    "6. **Statistical Summary**\n",
    "7. **Data Quality Assessment**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from scipy import stats\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# 🎨 Set Style and Configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 📊 Configure Plotting Parameters\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")\n",
    "print(f\"📅 Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1️⃣ Data Loading and Basic Information\n",
    "\n",
    "Let's start by loading the dataset and examining its basic structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📁 Load Dataset\n",
    "print(\"🔄 Loading APS Failure Training Dataset...\")\n",
    "data = pd.read_csv('aps_failure_training_set.csv', na_values=['na'])\n",
    "\n",
    "print(f\"✅ Dataset loaded successfully!\")\n",
    "print(f\"📏 Dataset Shape: {data.shape}\")\n",
    "print(f\"💾 Memory Usage: {data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# 📊 Display basic information\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"📋 DATASET INFORMATION\")\n",
    "print(\"=\"*50)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 👀 First Look at the Data\n",
    "print(\"🔍 First 5 rows of the dataset:\")\n",
    "display(data.head())\n",
    "\n",
    "print(\"\\n🔍 Last 5 rows of the dataset:\")\n",
    "display(data.tail())\n",
    "\n",
    "print(\"\\n📊 Dataset columns:\")\n",
    "print(f\"Total columns: {len(data.columns)}\")\n",
    "print(f\"Feature columns: {len(data.columns) - 1} (excluding target 'class')\")\n",
    "print(f\"Target column: 'class'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Class Distribution Analysis\n",
    "\n",
    "### 🚨 **Critical Finding**: Severe Class Imbalance\n",
    "The most important aspect of this dataset is the severe class imbalance. Let's analyze this in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Class Distribution Analysis\n",
    "class_counts = data['class'].value_counts()\n",
    "class_percentages = data['class'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"🎯 CLASS DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Total samples: {len(data):,}\")\n",
    "print(f\"\\nClass Counts:\")\n",
    "for class_name, count in class_counts.items():\n",
    "    percentage = class_percentages[class_name]\n",
    "    print(f\"  {class_name:>3}: {count:>6,} samples ({percentage:>5.2f}%)\")\n",
    "\n",
    "# 🔢 Calculate imbalance ratio\n",
    "majority_class = class_counts.index[0]\n",
    "minority_class = class_counts.index[1]\n",
    "imbalance_ratio = class_counts[majority_class] / class_counts[minority_class]\n",
    "\n",
    "print(f\"\\n⚖️ IMBALANCE METRICS:\")\n",
    "print(f\"  Majority class ('{majority_class}'): {class_counts[majority_class]:,} samples\")\n",
    "print(f\"  Minority class ('{minority_class}'): {class_counts[minority_class]:,} samples\")\n",
    "print(f\"  📈 Imbalance Ratio: {imbalance_ratio:.1f}:1\")\n",
    "print(f\"  🚨 Severity: {'EXTREME' if imbalance_ratio > 50 else 'HIGH' if imbalance_ratio > 10 else 'MODERATE'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Visualize Class Distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('🎯 Class Distribution Analysis - APS Failure Dataset', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Bar Plot - Absolute Counts\n",
    "ax1 = axes[0, 0]\n",
    "bars = ax1.bar(class_counts.index, class_counts.values, \n",
    "               color=['#FF6B6B', '#4ECDC4'], alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax1.set_title('📊 Absolute Class Counts', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Class', fontsize=12)\n",
    "ax1.set_ylabel('Number of Samples', fontsize=12)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, count in zip(bars, class_counts.values):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "             f'{count:,}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "# 2. Pie Chart - Percentage Distribution\n",
    "ax2 = axes[0, 1]\n",
    "colors = ['#FF6B6B', '#4ECDC4']\n",
    "wedges, texts, autotexts = ax2.pie(class_counts.values, labels=class_counts.index, \n",
    "                                   autopct='%1.2f%%', colors=colors, \n",
    "                                   startangle=90, explode=[0.05, 0])\n",
    "ax2.set_title('🥧 Class Distribution (%)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Enhance pie chart text\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontweight('bold')\n",
    "    autotext.set_fontsize(11)\n",
    "\n",
    "# 3. Log Scale Visualization\n",
    "ax3 = axes[1, 0]\n",
    "bars_log = ax3.bar(class_counts.index, class_counts.values, \n",
    "                   color=['#FF6B6B', '#4ECDC4'], alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax3.set_yscale('log')\n",
    "ax3.set_title('📈 Class Counts (Log Scale)', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Class', fontsize=12)\n",
    "ax3.set_ylabel('Number of Samples (Log Scale)', fontsize=12)\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on log bars\n",
    "for bar, count in zip(bars_log, class_counts.values):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height * 1.1,\n",
    "             f'{count:,}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "# 4. Imbalance Ratio Visualization\n",
    "ax4 = axes[1, 1]\n",
    "ratio_data = [imbalance_ratio, 1]\n",
    "bars_ratio = ax4.bar(['Majority:Minority', 'Baseline (1:1)'], ratio_data, \n",
    "                     color=['#FF6B6B', '#95A5A6'], alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax4.set_title(f'⚖️ Imbalance Ratio: {imbalance_ratio:.1f}:1', fontsize=14, fontweight='bold')\n",
    "ax4.set_ylabel('Ratio', fontsize=12)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add ratio labels\n",
    "for bar, ratio in zip(bars_ratio, ratio_data):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "             f'{ratio:.1f}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifacts/class_distribution_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"💾 Visualization saved to: artifacts/class_distribution_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Missing Values Analysis\n",
    "\n",
    "Understanding missing data patterns is crucial for preprocessing decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 Missing Values Analysis\n",
    "missing_data = data.isnull().sum()\n",
    "missing_percentage = (missing_data / len(data)) * 100\n",
    "missing_info = pd.DataFrame({\n",
    "    'Column': missing_data.index,\n",
    "    'Missing_Count': missing_data.values,\n",
    "    'Missing_Percentage': missing_percentage.values\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "# Filter columns with missing values\n",
    "missing_info_filtered = missing_info[missing_info['Missing_Count'] > 0]\n",
    "\n",
    "print(\"🔍 MISSING VALUES ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total columns: {len(data.columns)}\")\n",
    "print(f\"Columns with missing values: {len(missing_info_filtered)}\")\n",
    "print(f\"Columns without missing values: {len(data.columns) - len(missing_info_filtered)}\")\n",
    "\n",
    "if len(missing_info_filtered) > 0:\n",
    "    print(f\"\\n📊 Top 10 columns with missing values:\")\n",
    "    display(missing_info_filtered.head(10))\n",
    "else:\n",
    "    print(\"\\n✅ No missing values found in the dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Visualize Missing Values Pattern\n",
    "if len(missing_info_filtered) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "    fig.suptitle('🔍 Missing Values Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Missing values count (top 20)\n",
    "    ax1 = axes[0, 0]\n",
    "    top_missing = missing_info_filtered.head(20)\n",
    "    bars = ax1.barh(range(len(top_missing)), top_missing['Missing_Count'], \n",
    "                    color='#E74C3C', alpha=0.8)\n",
    "    ax1.set_yticks(range(len(top_missing)))\n",
    "    ax1.set_yticklabels(top_missing['Column'])\n",
    "    ax1.set_xlabel('Number of Missing Values')\n",
    "    ax1.set_title('📊 Top 20 Columns with Missing Values', fontweight='bold')\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        ax1.text(width + width*0.01, bar.get_y() + bar.get_height()/2,\n",
    "                f'{int(width):,}', ha='left', va='center', fontweight='bold')\n",
    "    \n",
    "    # 2. Missing values percentage\n",
    "    ax2 = axes[0, 1]\n",
    "    bars2 = ax2.barh(range(len(top_missing)), top_missing['Missing_Percentage'], \n",
    "                     color='#F39C12', alpha=0.8)\n",
    "    ax2.set_yticks(range(len(top_missing)))\n",
    "    ax2.set_yticklabels(top_missing['Column'])\n",
    "    ax2.set_xlabel('Missing Percentage (%)')\n",
    "    ax2.set_title('📈 Missing Values Percentage', fontweight='bold')\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, bar in enumerate(bars2):\n",
    "        width = bar.get_width()\n",
    "        ax2.text(width + width*0.01, bar.get_y() + bar.get_height()/2,\n",
    "                f'{width:.1f}%', ha='left', va='center', fontweight='bold')\n",
    "    \n",
    "    # 3. Missing values distribution\n",
    "    ax3 = axes[1, 0]\n",
    "    missing_dist = missing_info_filtered['Missing_Percentage']\n",
    "    ax3.hist(missing_dist, bins=20, color='#9B59B6', alpha=0.8, edgecolor='black')\n",
    "    ax3.set_xlabel('Missing Percentage (%)')\n",
    "    ax3.set_ylabel('Number of Columns')\n",
    "    ax3.set_title('📊 Distribution of Missing Values', fontweight='bold')\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 4. Missing values by class\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Calculate missing values by class for a few key columns\n",
    "    key_missing_cols = missing_info_filtered.head(5)['Column'].tolist()\n",
    "    class_missing = []\n",
    "    \n",
    "    for col in key_missing_cols:\n",
    "        for class_val in data['class'].unique():\n",
    "            class_data = data[data['class'] == class_val]\n",
    "            missing_count = class_data[col].isnull().sum()\n",
    "            missing_pct = (missing_count / len(class_data)) * 100\n",
    "            class_missing.append({\n",
    "                'Column': col,\n",
    "                'Class': class_val,\n",
    "                'Missing_Pct': missing_pct\n",
    "            })\n",
    "    \n",
    "    class_missing_df = pd.DataFrame(class_missing)\n",
    "    \n",
    "    # Create grouped bar plot\n",
    "    x_pos = np.arange(len(key_missing_cols))\n",
    "    width = 0.35\n",
    "    \n",
    "    neg_data = class_missing_df[class_missing_df['Class'] == 'neg']['Missing_Pct'].values\n",
    "    pos_data = class_missing_df[class_missing_df['Class'] == 'pos']['Missing_Pct'].values\n",
    "    \n",
    "    bars1 = ax4.bar(x_pos - width/2, neg_data, width, label='Negative', \n",
    "                    color='#3498DB', alpha=0.8)\n",
    "    bars2 = ax4.bar(x_pos + width/2, pos_data, width, label='Positive', \n",
    "                    color='#E74C3C', alpha=0.8)\n",
    "    \n",
    "    ax4.set_xlabel('Columns')\n",
    "    ax4.set_ylabel('Missing Percentage (%)')\n",
    "    ax4.set_title('📊 Missing Values by Class', fontweight='bold')\n",
    "    ax4.set_xticks(x_pos)\n",
    "    ax4.set_xticklabels(key_missing_cols, rotation=45, ha='right')\n",
    "    ax4.legend()\n",
    "    ax4.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('artifacts/missing_values_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"💾 Visualization saved to: artifacts/missing_values_analysis.png\")\n",
    "else:\n",
    "    print(\"✅ No missing values to visualize!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Feature Distribution Analysis\n",
    "\n",
    "Let's analyze the distribution of key features and their relationship with the target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Feature Distribution Analysis\n",
    "# Select numeric columns (excluding target)\n",
    "numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'class' in numeric_cols:\n",
    "    numeric_cols.remove('class')\n",
    "\n",
    "print(f\"📊 FEATURE DISTRIBUTION ANALYSIS\")\n",
    "print(f\"Total numeric features: {len(numeric_cols)}\")\n",
    "\n",
    "# Analyze key statistical measures\n",
    "feature_stats = data[numeric_cols].describe()\n",
    "print(\"\\n📈 Statistical Summary (Top 10 features):\")\n",
    "display(feature_stats.iloc[:, :10].round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Visualize Feature Distributions\n",
    "# Select first 12 features for visualization\n",
    "sample_features = numeric_cols[:12]\n",
    "\n",
    "fig, axes = plt.subplots(4, 3, figsize=(20, 16))\n",
    "fig.suptitle('📊 Feature Distribution Analysis (Sample Features)', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, feature in enumerate(sample_features):\n",
    "    row, col = i // 3, i % 3\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Remove zero values for better visualization\n",
    "    feature_data = data[feature].dropna()\n",
    "    non_zero_data = feature_data[feature_data != 0]\n",
    "    \n",
    "    if len(non_zero_data) > 0:\n",
    "        # Use log scale if data has high variance\n",
    "        if non_zero_data.std() > non_zero_data.mean() * 2:\n",
    "            ax.hist(np.log1p(non_zero_data), bins=30, color='skyblue', \n",
    "                   alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "            ax.set_xlabel(f'log({feature})')\n",
    "        else:\n",
    "            ax.hist(non_zero_data, bins=30, color='skyblue', \n",
    "                   alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "            ax.set_xlabel(feature)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No non-zero data', ha='center', va='center', \n",
    "               transform=ax.transAxes, fontsize=12)\n",
    "    \n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'{feature}', fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifacts/feature_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"💾 Visualization saved to: artifacts/feature_distributions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Feature Comparison by Class\n",
    "# Select a few key features for class comparison\n",
    "key_features = numeric_cols[:6]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.suptitle('🎯 Feature Comparison by Class', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, feature in enumerate(key_features):\n",
    "    row, col = i // 3, i % 3\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Separate data by class\n",
    "    neg_data = data[data['class'] == 'neg'][feature].dropna()\n",
    "    pos_data = data[data['class'] == 'pos'][feature].dropna()\n",
    "    \n",
    "    # Remove zeros for better visualization\n",
    "    neg_data_nz = neg_data[neg_data != 0]\n",
    "    pos_data_nz = pos_data[pos_data != 0]\n",
    "    \n",
    "    if len(neg_data_nz) > 0 and len(pos_data_nz) > 0:\n",
    "        # Use log scale for high variance data\n",
    "        if neg_data_nz.std() > neg_data_nz.mean() * 2:\n",
    "            ax.hist(np.log1p(neg_data_nz), bins=30, alpha=0.7, \n",
    "                   label='Negative', color='#3498DB', edgecolor='black', linewidth=0.5)\n",
    "            ax.hist(np.log1p(pos_data_nz), bins=30, alpha=0.7, \n",
    "                   label='Positive', color='#E74C3C', edgecolor='black', linewidth=0.5)\n",
    "            ax.set_xlabel(f'log({feature})')\n",
    "        else:\n",
    "            ax.hist(neg_data_nz, bins=30, alpha=0.7, \n",
    "                   label='Negative', color='#3498DB', edgecolor='black', linewidth=0.5)\n",
    "            ax.hist(pos_data_nz, bins=30, alpha=0.7, \n",
    "                   label='Positive', color='#E74C3C', edgecolor='black', linewidth=0.5)\n",
    "            ax.set_xlabel(feature)\n",
    "    \n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'{feature}', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifacts/feature_comparison_by_class.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"💾 Visualization saved to: artifacts/feature_comparison_by_class.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Correlation Analysis\n",
    "\n",
    "Understanding feature correlations helps identify redundant features and feature relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔗 Correlation Analysis\n",
    "print(\"🔗 CORRELATION ANALYSIS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Calculate correlation matrix for a subset of features (first 20)\n",
    "sample_features_corr = numeric_cols[:20]\n",
    "corr_data = data[sample_features_corr].corr()\n",
    "\n",
    "# Find highly correlated pairs\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_data.columns)):\n",
    "    for j in range(i+1, len(corr_data.columns)):\n",
    "        corr_val = corr_data.iloc[i, j]\n",
    "        if abs(corr_val) > 0.8:  # High correlation threshold\n",
    "            high_corr_pairs.append({\n",
    "                'Feature1': corr_data.columns[i],\n",
    "                'Feature2': corr_data.columns[j],\n",
    "                'Correlation': corr_val\n",
    "            })\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(f\"\\n🔗 Highly correlated feature pairs (|r| > 0.8):\")\n",
    "    high_corr_df = pd.DataFrame(high_corr_pairs).sort_values('Correlation', key=abs, ascending=False)\n",
    "    display(high_corr_df)\n",
    "else:\n",
    "    print(\"\\n✅ No highly correlated feature pairs found (|r| > 0.8)\")\n",
    "\n",
    "print(f\"\\n📊 Correlation matrix shape: {corr_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Visualize Correlation Matrix\n",
    "plt.figure(figsize=(16, 14))\n",
    "\n",
    "# Create correlation heatmap\n",
    "mask = np.triu(np.ones_like(corr_data, dtype=bool))  # Mask upper triangle\n",
    "heatmap = sns.heatmap(corr_data, mask=mask, annot=False, cmap='RdBu_r', center=0,\n",
    "                      square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "\n",
    "plt.title('🔗 Feature Correlation Matrix (Sample Features)', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Features', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifacts/correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"💾 Visualization saved to: artifacts/correlation_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Statistical Summary\n",
    "\n",
    "Comprehensive statistical analysis of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📈 Comprehensive Statistical Summary\n",
    "print(\"📈 COMPREHENSIVE STATISTICAL SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Dataset overview\n",
    "print(f\"📊 Dataset Overview:\")\n",
    "print(f\"  • Total samples: {len(data):,}\")\n",
    "print(f\"  • Total features: {len(numeric_cols)}\")\n",
    "print(f\"  • Memory usage: {data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Class distribution summary\n",
    "print(f\"\\n🎯 Class Distribution:\")\n",
    "for class_name, count in class_counts.items():\n",
    "    percentage = class_percentages[class_name]\n",
    "    print(f\"  • {class_name}: {count:,} ({percentage:.2f}%)\")\n",
    "print(f\"  • Imbalance ratio: {imbalance_ratio:.1f}:1\")\n",
    "\n",
    "# Missing values summary\n",
    "total_missing = data.isnull().sum().sum()\n",
    "missing_percentage_total = (total_missing / (len(data) * len(data.columns))) * 100\n",
    "print(f\"\\n🔍 Missing Values:\")\n",
    "print(f\"  • Total missing values: {total_missing:,}\")\n",
    "print(f\"  • Percentage of dataset: {missing_percentage_total:.2f}%\")\n",
    "print(f\"  • Columns with missing values: {len(missing_info_filtered)}\")\n",
    "\n",
    "# Feature statistics\n",
    "feature_stats_summary = data[numeric_cols].describe()\n",
    "print(f\"\\n📊 Feature Statistics Summary:\")\n",
    "print(f\"  • Mean of means: {feature_stats_summary.loc['mean'].mean():.2e}\")\n",
    "print(f\"  • Mean of std deviations: {feature_stats_summary.loc['std'].mean():.2e}\")\n",
    "print(f\"  • Features with zero minimum: {(feature_stats_summary.loc['min'] == 0).sum()}\")\n",
    "print(f\"  • Features with high skewness (>2): {len([col for col in numeric_cols if abs(data[col].skew()) > 2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7️⃣ Data Quality Assessment\n",
    "\n",
    "Final assessment of data quality and recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 Data Quality Assessment\n",
    "print(\"🔍 DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Initialize quality score\n",
    "quality_score = 100\n",
    "issues = []\n",
    "\n",
    "# Check class imbalance\n",
    "if imbalance_ratio > 50:\n",
    "    quality_score -= 30\n",
    "    issues.append(f\"🚨 Severe class imbalance ({imbalance_ratio:.1f}:1)\")\n",
    "elif imbalance_ratio > 10:\n",
    "    quality_score -= 20\n",
    "    issues.append(f\"⚠️ High class imbalance ({imbalance_ratio:.1f}:1)\")\n",
    "\n",
    "# Check missing values\n",
    "if missing_percentage_total > 20:\n",
    "    quality_score -= 25\n",
    "    issues.append(f\"🚨 High missing values ({missing_percentage_total:.1f}%)\")\n",
    "elif missing_percentage_total > 10:\n",
    "    quality_score -= 15\n",
    "    issues.append(f\"⚠️ Moderate missing values ({missing_percentage_total:.1f}%)\")\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_rows = data.duplicated().sum()\n",
    "if duplicate_rows > 0:\n",
    "    quality_score -= 10\n",
    "    issues.append(f\"⚠️ Duplicate rows found: {duplicate_rows}\")\n",
    "\n",
    "# Check for constant features\n",
    "constant_features = []\n",
    "for col in numeric_cols:\n",
    "    if data[col].nunique() <= 1:\n",
    "        constant_features.append(col)\n",
    "\n",
    "if len(constant_features) > 0:\n",
    "    quality_score -= 10\n",
    "    issues.append(f\"⚠️ Constant features: {len(constant_features)}\")\n",
    "\n",
    "# Print assessment results\n",
    "print(f\"📊 OVERALL DATA QUALITY SCORE: {quality_score}/100\")\n",
    "\n",
    "if quality_score >= 80:\n",
    "    print(\"✅ EXCELLENT data quality\")\n",
    "elif quality_score >= 60:\n",
    "    print(\"🟡 GOOD data quality with minor issues\")\n",
    "elif quality_score >= 40:\n",
    "    print(\"🟠 FAIR data quality with notable issues\")\n",
    "else:\n",
    "    print(\"🔴 POOR data quality requiring significant preprocessing\")\n",
    "\n",
    "print(f\"\\n🔍 Identified Issues:\")\n",
    "for i, issue in enumerate(issues, 1):\n",
    "    print(f\"  {i}. {issue}\")\n",
    "\n",
    "if not issues:\n",
    "    print(\"  ✅ No major data quality issues identified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 EDA Summary & Recommendations\n",
    "\n",
    "### 🎯 **Key Findings:**\n",
    "\n",
    "1. **🚨 Severe Class Imbalance**: The dataset has an extreme imbalance ratio of approximately 59:1 (negative:positive)\n",
    "2. **📊 Large Feature Space**: 170 anonymous features require careful preprocessing\n",
    "3. **🔍 Missing Values**: Significant missing data patterns that need handling\n",
    "4. **📈 High Variance Features**: Many features show high variance requiring scaling\n",
    "\n",
    "### 💡 **Recommendations for Next Steps:**\n",
    "\n",
    "1. **🎯 Class Imbalance Handling**:\n",
    "   - Implement SMOTE (Synthetic Minority Oversampling Technique)\n",
    "   - Use class weights in algorithms\n",
    "   - Consider ensemble methods\n",
    "   - Focus on F1-score and AUC metrics instead of accuracy\n",
    "\n",
    "2. **🔧 Preprocessing Requirements**:\n",
    "   - Handle missing values (median imputation recommended)\n",
    "   - Scale features (StandardScaler or MinMaxScaler)\n",
    "   - Remove or handle constant features\n",
    "   - Consider feature selection techniques\n",
    "\n",
    "3. **📊 Model Selection**:\n",
    "   - Start with algorithms robust to imbalance (Random Forest, XGBoost)\n",
    "   - Implement proper cross-validation with stratification\n",
    "   - Use appropriate evaluation metrics for imbalanced data\n",
    "\n",
    "---\n",
    "\n",
    "**✅ EDA Complete! Ready for Data Cleaning and Preprocessing.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
